{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. homework for BME Deep Learning course\n",
    "\n",
    "Based on: https://github.com/BME-SmartLab-Education/vitmav45/blob/master/11/11-02-Word-Embeddings-Keras.ipynb\n",
    "& https://github.com/BME-SmartLab-Education/vitmav45-2016-Epochalypse/blob/master/Final_Neural_Networks/1D_CNN_LSTM.ipynb\n",
    "\n",
    "Written by Mor√≥ Anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model,Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "import random\n",
    "import sys\n",
    "\n",
    "BASE_DIR = '.'\n",
    "GLOVE_DIR = BASE_DIR + '/glove.6B'\n",
    "TEXT_DATA_DIR = BASE_DIR + '/books'\n",
    "MAX_NB_WORDS = 20000      # Maximum number of different words\n",
    "EMBEDDING_DIM = 100       # Dimension of the embedding\n",
    "LENGTH_OF_CHUNKS = 100    # Length of the text-pieces used for the neural network (100 words/sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded embeddings: 400000\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Number of loaded embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the books\n",
    "\n",
    "I will use 9 books, 3 books from 3 authors:\n",
    "    Jane Austen: Pride and Prejudice, Sense and Sensibility, Mansfield Park;\n",
    "    Edgar Allan Poe: The masque of the red death, The cask of amontillado, The fall of the house Usher;\n",
    "    H. G. Wells: The time machine, The island of Dr Moreau, The red room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen\n",
      "0\n",
      "./books/austen/mansfield.txt\n",
      "./books/austen/pride.txt\n",
      "./books/austen/sense.txt\n",
      "poe\n",
      "1\n",
      "./books/poe/cask.txt\n",
      "./books/poe/fall.txt\n",
      "./books/poe/masque.txt\n",
      "wells\n",
      "2\n",
      "./books/wells/island.txt\n",
      "./books/wells/redRoom.txt\n",
      "./books/wells/timeMachine.txt\n",
      "Number of books:  9\n"
     ]
    }
   ],
   "source": [
    "texts = []         # text of books\n",
    "labels_index = {}  # IDs of the authors\n",
    "labels = []        # for the storage of IDs\n",
    "\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    print (name)\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        print(label_id)\n",
    "        labels_index[name] = label_id # new ID for a new author\n",
    "        for fname in sorted(os.listdir(path)):            \n",
    "            fpath = os.path.join(path, fname)\n",
    "            print(fpath)\n",
    "            f = open(fpath)\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            labels.append(label_id)\n",
    "\n",
    "print('Number of books: ', len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of different words found in all of the books:  19657\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('The number of different words found in all of the books: ', len(word_index))\n",
    "\n",
    "#One-hot coding for the labels (author IDs)\n",
    "labels = to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to cut up the \"texts\" into smaller parts\n",
    "\n",
    "(The length of the pieces can be set with the variable LENGTH_OF_CHUNKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cutUp(sequence): \n",
    "    \n",
    "    # This line does the actual cutting:\n",
    "    data = [sequence[i:i + LENGTH_OF_CHUNKS] for i in range(0, len(sequence), LENGTH_OF_CHUNKS)]\n",
    "    \n",
    "    # If the last line of the samples is smaller than the given length, it will be thrown away\n",
    "    if(len(sequence) / LENGTH_OF_CHUNKS != 0):\n",
    "        data = np.delete(data, (len(data)-1), axis=0)  \n",
    "        \n",
    "    return data, len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the dataset and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  (5173,)\n",
      "Labels:  (5173, 3)\n"
     ]
    }
   ],
   "source": [
    "x_data = []     # input data \n",
    "y = []          #labels\n",
    "\n",
    "for i in range (0, 9):\n",
    "    data_raw, length = cutUp(sequences[i])  # Cut into smaller parts\n",
    "    x_data = np.append(x_data, np.squeeze(np.asarray(data_raw)))\n",
    "    x_data = np.squeeze(np.asarray(x_data))\n",
    "    \n",
    "    for j in range (0,length):    # Fill up an array with the labels (in the same order as the data pieces)\n",
    "        y[len(y):] = [labels[i]]\n",
    "    y_data = np.squeeze(np.asarray(y))\n",
    "    \n",
    "print(\"Input data: \", x_data.shape)\n",
    "print(\"Labels: \", y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide the dataset into train, validation and test data\n",
    "\n",
    "Train data: 75%, validation and test data: 15-15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To shuffle the data, shuffle a vector which contains the available indices\n",
    "indices = np.arange(x_data.shape[0]) \n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# The shuffle:\n",
    "data = x_data[indices]\n",
    "labels = y_data[indices]\n",
    "\n",
    "# The number of samples of train, test and validation\n",
    "nb_train_samples = int(len(data)*0.7)\n",
    "nb_val_samples = int(len(data)*0.15)\n",
    "nb_test_samples = int(len(data)*0.15)\n",
    "\n",
    "train_end = nb_train_samples\n",
    "val_end = nb_train_samples + nb_val_samples\n",
    "\n",
    "x_tr = data[:train_end]\n",
    "y_train = labels[:train_end]\n",
    "x_v = data[train_end:val_end]\n",
    "y_val = labels[train_end:val_end]\n",
    "x_t = data[val_end:]\n",
    "y_test = labels[val_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape the data matrices\n",
    "\n",
    "In this shape the data can't be processed by a neural network, it has to be rearranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (3621, 100)\n",
      "Validation data shape:  (775, 100)\n",
      "Test data shape:  (777, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.zeros((x_tr.size, LENGTH_OF_CHUNKS))\n",
    "x_val = np.zeros((x_v.size, LENGTH_OF_CHUNKS))\n",
    "x_test = np.zeros((x_t.size, LENGTH_OF_CHUNKS))\n",
    "\n",
    "for i in range (0, x_tr.size):\n",
    "    x_train[i] = x_tr[i]\n",
    "    \n",
    "for i in range (0, x_v.size):\n",
    "    x_val[i] = x_v[i]\n",
    "    \n",
    "for i in range (0, x_t.size):\n",
    "    x_test[i] = x_t[i]\n",
    "    \n",
    "print(\"Train data shape: \", x_train.shape)\n",
    "print(\"Validation data shape: \", x_val.shape)\n",
    "print(\"Test data shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use embedding on the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines of the embedding matrix: 19658\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM)) \n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print ('Number of lines of the embedding matrix:', len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the RNN\n",
    "\n",
    "It contains an embedding layer, two sets of 1D convolutional layer-max-pooling-dropout, an LSTM after these and a fully connected layer with softmax by the end. Also, early stopping is used during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=LENGTH_OF_CHUNKS,\n",
    "                            trainable=False))    # In the first turn, the embedding layers weights' will be freezed\n",
    "\n",
    "model.add(Conv1D(nb_filter=128,filter_length=5, border_mode='valid', activation='relu', subsample_length=2))\n",
    "model.add(MaxPooling1D(pool_length=5))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(nb_filter=128,filter_length=5, border_mode='valid', activation='relu', subsample_length=2))\n",
    "model.add(MaxPooling1D(pool_length=3))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3621 samples, validate on 775 samples\n",
      "Epoch 1/15\n",
      "3621/3621 [==============================] - 4s - loss: 0.6009 - acc: 0.8001 - val_loss: 0.5771 - val_acc: 0.8052\n",
      "Epoch 2/15\n",
      "3621/3621 [==============================] - 3s - loss: 0.5662 - acc: 0.8108 - val_loss: 0.5599 - val_acc: 0.8052\n",
      "Epoch 3/15\n",
      "3621/3621 [==============================] - 4s - loss: 0.4514 - acc: 0.8313 - val_loss: 0.2106 - val_acc: 0.9432\n",
      "Epoch 4/15\n",
      "3621/3621 [==============================] - 4s - loss: 0.2021 - acc: 0.9373 - val_loss: 0.1805 - val_acc: 0.9432\n",
      "Epoch 5/15\n",
      "3621/3621 [==============================] - 4s - loss: 0.1600 - acc: 0.9514 - val_loss: 0.1482 - val_acc: 0.9600\n",
      "Epoch 6/15\n",
      "3621/3621 [==============================] - 3s - loss: 0.1470 - acc: 0.9572 - val_loss: 0.1651 - val_acc: 0.9432\n",
      "Epoch 7/15\n",
      "3621/3621 [==============================] - 3s - loss: 0.1303 - acc: 0.9600 - val_loss: 0.1355 - val_acc: 0.9626\n",
      "Epoch 8/15\n",
      "3621/3621 [==============================] - 3s - loss: 0.1148 - acc: 0.9633 - val_loss: 0.1272 - val_acc: 0.9626\n",
      "Epoch 9/15\n",
      "3621/3621 [==============================] - 3s - loss: 0.1023 - acc: 0.9674 - val_loss: 0.1237 - val_acc: 0.9613\n",
      "Epoch 10/15\n",
      "3621/3621 [==============================] - 3s - loss: 0.0975 - acc: 0.9685 - val_loss: 0.1155 - val_acc: 0.9665\n",
      "Epoch 11/15\n",
      "3621/3621 [==============================] - 4s - loss: 0.0928 - acc: 0.9699 - val_loss: 0.1275 - val_acc: 0.9652\n",
      "Epoch 12/15\n",
      "3621/3621 [==============================] - 3s - loss: 0.0858 - acc: 0.9705 - val_loss: 0.1122 - val_acc: 0.9665\n",
      "Epoch 13/15\n",
      "3621/3621 [==============================] - 4s - loss: 0.0813 - acc: 0.9727 - val_loss: 0.1096 - val_acc: 0.9665\n",
      "Epoch 14/15\n",
      "3621/3621 [==============================] - 4s - loss: 0.0672 - acc: 0.9771 - val_loss: 0.1462 - val_acc: 0.9535\n",
      "Epoch 15/15\n",
      "3621/3621 [==============================] - 4s - loss: 0.0612 - acc: 0.9823 - val_loss: 0.1048 - val_acc: 0.9677\n",
      "768/777 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13618943357493782, 0.9575289575289575]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, callbacks=[earlyStopping], validation_data=(x_val, y_val), nb_epoch=15, batch_size=50)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After it, the embedding layer will be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3621 samples, validate on 775 samples\n",
      "Epoch 1/10\n",
      "3621/3621 [==============================] - 8s - loss: 0.0539 - acc: 0.9843 - val_loss: 0.1275 - val_acc: 0.9677\n",
      "Epoch 2/10\n",
      "3621/3621 [==============================] - 6s - loss: 0.0443 - acc: 0.9870 - val_loss: 0.1367 - val_acc: 0.9665\n",
      "Epoch 3/10\n",
      "3621/3621 [==============================] - 7s - loss: 0.0285 - acc: 0.9936 - val_loss: 0.1216 - val_acc: 0.9677\n",
      "Epoch 4/10\n",
      "3621/3621 [==============================] - 6s - loss: 0.0228 - acc: 0.9936 - val_loss: 0.1229 - val_acc: 0.9690\n",
      "Epoch 5/10\n",
      "3621/3621 [==============================] - 6s - loss: 0.0150 - acc: 0.9959 - val_loss: 0.1507 - val_acc: 0.9639\n",
      "Epoch 6/10\n",
      "3621/3621 [==============================] - 6s - loss: 0.0172 - acc: 0.9936 - val_loss: 0.1160 - val_acc: 0.9729\n",
      "Epoch 7/10\n",
      "3621/3621 [==============================] - 6s - loss: 0.0056 - acc: 0.9983 - val_loss: 0.1065 - val_acc: 0.9768\n",
      "Epoch 8/10\n",
      "3621/3621 [==============================] - 6s - loss: 0.0060 - acc: 0.9983 - val_loss: 0.1007 - val_acc: 0.9768\n",
      "Epoch 9/10\n",
      "3621/3621 [==============================] - 7s - loss: 0.0045 - acc: 0.9983 - val_loss: 0.1220 - val_acc: 0.9768\n",
      "Epoch 10/10\n",
      "3621/3621 [==============================] - 7s - loss: 0.0080 - acc: 0.9978 - val_loss: 0.1096 - val_acc: 0.9768\n",
      "768/777 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14291747211788006, 0.96782496782496785]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].trainable=True # This allows the embedding layer to be trained\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train, callbacks=[earlyStopping], validation_data=(x_val, y_val), nb_epoch=10, batch_size=50)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
