{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model,Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import random\n",
    "import sys\n",
    "\n",
    "BASE_DIR = '.'\n",
    "GLOVE_DIR = BASE_DIR + '/GloVe-1.2/glove.6B/'\n",
    "TEXT_DATA_DIR = BASE_DIR + '/books'\n",
    "MAX_NB_WORDS = 20000 # Ennyi különböző szót kezelünk majd\n",
    "EMBEDDING_DIM = 100 # Ekkora lesz a használt beágyazás\n",
    "LENGTH_OF_CHUNKS = 100 # Ekkora lesz egy-egy minta a szovegbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Betöltött beágyazásaok száma: 400000\n",
      "A `the` beágyazó vektorának első 10 eleme: [-0.038194   -0.24487001  0.72812003 -0.39961001  0.083172    0.043953\n",
      " -0.39140999  0.3344     -0.57545     0.087459  ]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split() # \"tokenizáljuk\" a sort\n",
    "    word = values[0] # maga a szó\n",
    "    coefs = np.asarray(values[1:], dtype='float32') # a szót követő beágyazás 100 koordinátán\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Betöltött beágyazások száma:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen\n",
      "0\n",
      "./books/austen/mansfield.txt\n",
      "./books/austen/pride.txt\n",
      "./books/austen/sense.txt\n",
      "poe\n",
      "1\n",
      "./books/poe/cask.txt\n",
      "./books/poe/fall.txt\n",
      "./books/poe/masque.txt\n",
      "wells\n",
      "2\n",
      "./books/wells/island.txt\n",
      "./books/wells/redRoom.txt\n",
      "./books/wells/timeMachine.txt\n",
      "Number of books:  9\n"
     ]
    }
   ],
   "source": [
    "texts = []         # text of books\n",
    "labels_index = {}  # IDs of the authors\n",
    "labels = []        # for the storage of IDs\n",
    "\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    print (name)\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        print(label_id)\n",
    "        labels_index[name] = label_id # new ID for a new author\n",
    "        for fname in sorted(os.listdir(path)):            \n",
    "            fpath = os.path.join(path, fname)\n",
    "            print(fpath)\n",
    "            f = open(fpath)\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            labels.append(label_id)\n",
    "\n",
    "print('Number of books: ', len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "926649\n",
      "725522\n",
      "705197\n",
      "32530\n",
      "62470\n",
      "33227\n",
      "266106\n",
      "41319\n",
      "201262\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,9):\n",
    "    print(len(texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Különböző szavak száma az összes szövegben:  19909\n",
      "A label tenzor alakja: (9, 3)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Különböző szavak száma az összes szövegben: ', len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('A label tenzor alakja:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169515\n",
      "131382\n",
      "128410\n",
      "5765\n",
      "10637\n",
      "5711\n",
      "50524\n",
      "7365\n",
      "36951\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,9):\n",
    "    print(len(sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cutUp(s): \n",
    "    sequence = np.squeeze(np.asarray(s))\n",
    "    data = [sequence[i:i + LENGTH_OF_CHUNKS] for i in range(0, len(sequence), LENGTH_OF_CHUNKS)] \n",
    "    if(len(sequence) / LENGTH_OF_CHUNKS != 0):\n",
    "        data = np.delete(data, (len(data)-1), axis=0)\n",
    "    return data, len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (1695,)\n",
      "Y:  (1695, 3)\n",
      "X:  (3008,)\n",
      "Y:  (3008, 3)\n",
      "X:  (4292,)\n",
      "Y:  (4292, 3)\n",
      "X:  (4349,)\n",
      "Y:  (4349, 3)\n",
      "X:  (4455,)\n",
      "Y:  (4455, 3)\n",
      "X:  (4512,)\n",
      "Y:  (4512, 3)\n",
      "X:  (5017,)\n",
      "Y:  (5017, 3)\n",
      "X:  (5090,)\n",
      "Y:  (5090, 3)\n",
      "X:  (5459,)\n",
      "Y:  (5459, 3)\n"
     ]
    }
   ],
   "source": [
    "x_data = []     # input data \n",
    "y = []          #labels\n",
    "for i in range (0, 9):\n",
    "    data_raw, length = cutUp(sequences[i])\n",
    "    x_data = np.append(x_data, np.squeeze(np.asarray(data_raw)))\n",
    "    x_data = np.squeeze(np.asarray(x_data))\n",
    "    \n",
    "    for j in range (0,length):\n",
    "        y[len(y):] = [labels[i]]\n",
    "    y_data = np.squeeze(np.asarray(y))\n",
    "    \n",
    "    print(\"X: \", x_data.shape)\n",
    "    print(\"Y: \", y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    3  1050     5    11     4   980    44    18   109  3028    21     1\n",
      "  4583     5     1  2734    67  1226   276  1147    21     9   793   124\n",
      "    83    72   168    56   198    67  1212    13  2843   156   973    20\n",
      "     1   684    32     2   131   231    35    15   167    33  5734     3\n",
      "  8800     6  1607   210 18978     3  1498    11    10     6   695   716\n",
      "     6   506    33   370  1666    62    18 18559  1107    20    39    52\n",
      "    44    18  2783  8538     5   500     8   637     9    38     9   185\n",
      "    17   134     5    22    88  3372   616     5    22   128  2824     5\n",
      "   871     8   228  2077] [ 1.  0.  0.]\n",
      "[  54   15  162 2858    4   14   20 2286 1590 3686 3873 4175  359 4238 1463\n",
      "  149   63   14  167  532    5    1 1087    5  599 2508    2    2  164  617\n",
      "  630 1080  279    5 1189   39  704   63   14 1888   98 1492    8   36  298\n",
      "  138  288 2498  319    5 1013   11   14 1156  760    6  704    5    1  689\n",
      "   63   57   14  678   20   11   30 1611   98  775 1099    4    1  339   14\n",
      "  352    1  138   38   63  255  352    1  138   28    6 1729 1442   14   62\n",
      "  322    1 1442  207   59  775 1099    1  339   39] [ 0.  1.  0.]\n",
      "[   34     4 12376    46    86     2     2 10847     7  1474    12     1\n",
      "  7194  2712  7070     5     1  6187    19  1673    12     6 15176     6\n",
      "  5480    19   461     8     1  7351   139   147    36  2271  1985    20\n",
      "     6   859   208     7  1143  4824    24   454  2354    12    10  2975\n",
      "   150     1   129     3    81     7  6703   175    53  9779    10   703\n",
      "   385     1  1865    39     1  6229 11241   127  1333   878     1 19501\n",
      "  3361  1539    24   118     7   317    11     4   162     1  1865    23\n",
      "    52    26    71     4  6470    34     4   252    12    48   139   249\n",
      "   147    10     1 14225] [ 0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(x_data[10], y_data[10])\n",
    "print(x_data[4500], y_data[4500])\n",
    "print(x_data[5400], y_data[5400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.arange(x_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = x_data[indices]\n",
    "labels = y_data[indices]\n",
    "\n",
    "nb_train_samples = int(len(data)*0.7)\n",
    "nb_val_samples = int(len(data)*0.15)\n",
    "nb_test_samples = int(len(data)*0.15)\n",
    "\n",
    "train_end = nb_train_samples\n",
    "val_end = nb_train_samples + nb_val_samples\n",
    "\n",
    "x_tr = data[:train_end]\n",
    "y_train = labels[:train_end]\n",
    "x_v = data[train_end:val_end]\n",
    "y_val = labels[train_end:val_end]\n",
    "x_t = data[val_end:]\n",
    "y_test = labels[val_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3821,) (3821, 3)\n",
      "(818,) (818, 3)\n",
      "(820,) (820, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_tr.shape, y_train.shape)\n",
    "print(x_v.shape, y_val.shape)\n",
    "print(x_t.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[3820].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (3821, 100)\n",
      "Validation data shape:  (818, 100)\n",
      "Test data shape:  (820, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.zeros((x_tr.size, LENGTH_OF_CHUNKS))\n",
    "x_val = np.zeros((x_v.size, LENGTH_OF_CHUNKS))\n",
    "x_test = np.zeros((x_t.size, LENGTH_OF_CHUNKS))\n",
    "\n",
    "for i in range (0, x_tr.size):\n",
    "    x_train[i] = x_tr[i]\n",
    "    \n",
    "for i in range (0, x_v.size):\n",
    "    x_val[i] = x_v[i]\n",
    "    \n",
    "for i in range (0, x_t.size):\n",
    "    x_test[i] = x_t[i]\n",
    "    \n",
    "print(\"Train data shape: \", x_train.shape)\n",
    "print(\"Validation data shape: \", x_val.shape)\n",
    "print(\"Test data shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A szövegre alkalmazott beágyazási mátrix sorainak száma: 19910\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM)) \n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print ('A szövegre alkalmazott beágyazási mátrix sorainak száma:', len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=LENGTH_OF_CHUNKS,\n",
    "                            trainable=False))\n",
    "\n",
    "model.add(Conv1D(nb_filter=128,filter_length=5, border_mode='valid', activation='relu', subsample_length=2))\n",
    "model.add(MaxPooling1D(pool_length=5))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(nb_filter=128,filter_length=5, border_mode='valid', activation='relu', subsample_length=2))\n",
    "model.add(MaxPooling1D(pool_length=3))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3821 samples, validate on 818 samples\n",
      "Epoch 1/5\n",
      "3821/3821 [==============================] - 6s - loss: 0.6693 - acc: 0.7752 - val_loss: 0.6547 - val_acc: 0.7775\n",
      "Epoch 2/5\n",
      "3821/3821 [==============================] - 4s - loss: 0.6201 - acc: 0.7838 - val_loss: 0.4846 - val_acc: 0.7775\n",
      "Epoch 3/5\n",
      "3821/3821 [==============================] - 4s - loss: 0.3210 - acc: 0.8728 - val_loss: 0.2615 - val_acc: 0.8985\n",
      "Epoch 4/5\n",
      "3821/3821 [==============================] - 4s - loss: 0.2251 - acc: 0.9197 - val_loss: 0.2160 - val_acc: 0.9205\n",
      "Epoch 5/5\n",
      "3821/3821 [==============================] - 5s - loss: 0.1969 - acc: 0.9254 - val_loss: 0.2162 - val_acc: 0.9218\n",
      "820/820 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2104394991223405, 0.91463414663221776]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=5, batch_size=50)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3821 samples, validate on 818 samples\n",
      "Epoch 1/5\n",
      "3821/3821 [==============================] - 9s - loss: 0.1722 - acc: 0.9335 - val_loss: 0.1737 - val_acc: 0.9328\n",
      "Epoch 2/5\n",
      "3821/3821 [==============================] - 7s - loss: 0.1459 - acc: 0.9398 - val_loss: 0.1856 - val_acc: 0.9315\n",
      "Epoch 3/5\n",
      "3821/3821 [==============================] - 8s - loss: 0.1241 - acc: 0.9463 - val_loss: 0.1999 - val_acc: 0.9242\n",
      "Epoch 4/5\n",
      "3821/3821 [==============================] - 8s - loss: 0.1132 - acc: 0.9521 - val_loss: 0.1651 - val_acc: 0.9328\n",
      "Epoch 5/5\n",
      "3821/3821 [==============================] - 7s - loss: 0.0947 - acc: 0.9579 - val_loss: 0.1880 - val_acc: 0.9267\n",
      "820/820 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1839982146170081, 0.91463414663221776]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].trainable=True\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=5, batch_size=50)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
