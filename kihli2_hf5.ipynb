{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. homework for BME Deep Learning course\n",
    "\n",
    "Based on: https://github.com/BME-SmartLab-Education/vitmav45/blob/master/11/11-02-Word-Embeddings-Keras.ipynb\n",
    "& https://github.com/BME-SmartLab-Education/vitmav45-2016-Epochalypse/blob/master/Final_Neural_Networks/1D_CNN_LSTM.ipynb\n",
    "\n",
    "Written by MorÃ³ Anna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model,Sequential\n",
    "from keras.utils.data_utils import get_file\n",
    "import random\n",
    "import sys\n",
    "\n",
    "BASE_DIR = '.'\n",
    "GLOVE_DIR = BASE_DIR + '/GloVe-1.2/glove.6B/'\n",
    "TEXT_DATA_DIR = BASE_DIR + '/books'\n",
    "MAX_NB_WORDS = 20000      # Maximum number of different words\n",
    "EMBEDDING_DIM = 100       # Dimension of the embedding\n",
    "LENGTH_OF_CHUNKS = 100    # Length of the text-pieces used for the neural network (100 words/sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded embeddings: 400000\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Number of loaded embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the books\n",
    "\n",
    "I will use 9 books, 3 books from 3 authors:\n",
    "    Jane Austen: Pride and Prejudice, Sense and Sensibility, Mansfield Park;\n",
    "    Edgar Allan Poe: The masque of the red death, The cask of amontillado, The fall of the house Usher;\n",
    "    H. G. Wells: The time machine, The island of Dr Moreau, The red room"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen\n",
      "0\n",
      "./books/austen/mansfield.txt\n",
      "./books/austen/pride.txt\n",
      "./books/austen/sense.txt\n",
      "poe\n",
      "1\n",
      "./books/poe/cask.txt\n",
      "./books/poe/fall.txt\n",
      "./books/poe/masque.txt\n",
      "wells\n",
      "2\n",
      "./books/wells/island.txt\n",
      "./books/wells/redRoom.txt\n",
      "./books/wells/timeMachine.txt\n",
      "Number of books:  9\n"
     ]
    }
   ],
   "source": [
    "texts = []         # text of books\n",
    "labels_index = {}  # IDs of the authors\n",
    "labels = []        # for the storage of IDs\n",
    "\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    print (name)\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        print(label_id)\n",
    "        labels_index[name] = label_id # new ID for a new author\n",
    "        for fname in sorted(os.listdir(path)):            \n",
    "            fpath = os.path.join(path, fname)\n",
    "            print(fpath)\n",
    "            f = open(fpath)\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            labels.append(label_id)\n",
    "\n",
    "print('Number of books: ', len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of different words found in all of the books:  19909\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('The number of different words found in all of the books: ', len(word_index))\n",
    "\n",
    "#One-hot coding for the labels (author IDs)\n",
    "labels = to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to cut up the \"texts\" into smaller parts\n",
    "\n",
    "(The length of the pieces can be set with the variable LENGTH_OF_CHUNKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cutUp(sequence): \n",
    "    \n",
    "    # This line does the actual cutting:\n",
    "    data = [sequence[i:i + LENGTH_OF_CHUNKS] for i in range(0, len(sequence), LENGTH_OF_CHUNKS)]\n",
    "    \n",
    "    # If the last line of the samples is smaller than the given length, it will be thrown away\n",
    "    if(len(sequence) / LENGTH_OF_CHUNKS != 0):\n",
    "        data = np.delete(data, (len(data)-1), axis=0)  \n",
    "        \n",
    "    return data, len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the dataset and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  (5459,)\n",
      "Labels:  (5459, 3)\n"
     ]
    }
   ],
   "source": [
    "x_data = []     # input data \n",
    "y = []          #labels\n",
    "\n",
    "for i in range (0, 9):\n",
    "    data_raw, length = cutUp(sequences[i])  # Cut into smaller parts\n",
    "    x_data = np.append(x_data, np.squeeze(np.asarray(data_raw)))\n",
    "    x_data = np.squeeze(np.asarray(x_data))\n",
    "    \n",
    "    for j in range (0,length):    # Fill up an array with the labels (in the same order as the data pieces)\n",
    "        y[len(y):] = [labels[i]]\n",
    "    y_data = np.squeeze(np.asarray(y))\n",
    "    \n",
    "print(\"Input data: \", x_data.shape)\n",
    "print(\"Labels: \", y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide the dataset into train, validation and test data\n",
    "\n",
    "Train data: 75%, validation and test data: 15-15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To shuffle the data, shuffle a vector which contains the available indices\n",
    "indices = np.arange(x_data.shape[0]) \n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# The shuffle:\n",
    "data = x_data[indices]\n",
    "labels = y_data[indices]\n",
    "\n",
    "# The number of samples of train, test and validation\n",
    "nb_train_samples = int(len(data)*0.7)\n",
    "nb_val_samples = int(len(data)*0.15)\n",
    "nb_test_samples = int(len(data)*0.15)\n",
    "\n",
    "train_end = nb_train_samples\n",
    "val_end = nb_train_samples + nb_val_samples\n",
    "\n",
    "x_tr = data[:train_end]\n",
    "y_train = labels[:train_end]\n",
    "x_v = data[train_end:val_end]\n",
    "y_val = labels[train_end:val_end]\n",
    "x_t = data[val_end:]\n",
    "y_test = labels[val_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape the data matrices\n",
    "\n",
    "In this shape the data can't be processed by a neural network, it has to be rearranged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (3821, 100)\n",
      "Validation data shape:  (818, 100)\n",
      "Test data shape:  (820, 100)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.zeros((x_tr.size, LENGTH_OF_CHUNKS))\n",
    "x_val = np.zeros((x_v.size, LENGTH_OF_CHUNKS))\n",
    "x_test = np.zeros((x_t.size, LENGTH_OF_CHUNKS))\n",
    "\n",
    "for i in range (0, x_tr.size):\n",
    "    x_train[i] = x_tr[i]\n",
    "    \n",
    "for i in range (0, x_v.size):\n",
    "    x_val[i] = x_v[i]\n",
    "    \n",
    "for i in range (0, x_t.size):\n",
    "    x_test[i] = x_t[i]\n",
    "    \n",
    "print(\"Train data shape: \", x_train.shape)\n",
    "print(\"Validation data shape: \", x_val.shape)\n",
    "print(\"Test data shape: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use embedding on the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines of the embedding matrix: 19910\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM)) \n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print ('Number of lines of the embedding matrix:', len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the RNN\n",
    "\n",
    "It contains an embedding layer, two sets of 1D convolutional layer-max-pooling-dropout, an LSTM after these and a fully connected layer with softmax by the end. Also, early stopping is used during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "earlyStopping=keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=LENGTH_OF_CHUNKS,\n",
    "                            trainable=False))    # In the first turn, the embedding layers weights' will be freezed\n",
    "\n",
    "model.add(Conv1D(nb_filter=128,filter_length=5, border_mode='valid', activation='relu', subsample_length=2))\n",
    "model.add(MaxPooling1D(pool_length=5))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(nb_filter=128,filter_length=5, border_mode='valid', activation='relu', subsample_length=2))\n",
    "model.add(MaxPooling1D(pool_length=3))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3821 samples, validate on 818 samples\n",
      "Epoch 1/15\n",
      "3821/3821 [==============================] - 5s - loss: 0.6625 - acc: 0.7843 - val_loss: 0.5951 - val_acc: 0.7971\n",
      "Epoch 2/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.6373 - acc: 0.7833 - val_loss: 0.5780 - val_acc: 0.7971\n",
      "Epoch 3/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.4376 - acc: 0.8338 - val_loss: 0.2288 - val_acc: 0.9193\n",
      "Epoch 4/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.2583 - acc: 0.9055 - val_loss: 0.1708 - val_acc: 0.9352\n",
      "Epoch 5/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.2051 - acc: 0.9238 - val_loss: 0.1537 - val_acc: 0.9364\n",
      "Epoch 6/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.1851 - acc: 0.9257 - val_loss: 0.1413 - val_acc: 0.9413\n",
      "Epoch 7/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.1634 - acc: 0.9338 - val_loss: 0.1493 - val_acc: 0.9328\n",
      "Epoch 8/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.1550 - acc: 0.9340 - val_loss: 0.1541 - val_acc: 0.9303\n",
      "Epoch 9/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.1420 - acc: 0.9406 - val_loss: 0.1522 - val_acc: 0.9279\n",
      "Epoch 10/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.1297 - acc: 0.9429 - val_loss: 0.1469 - val_acc: 0.9352\n",
      "Epoch 11/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.1195 - acc: 0.9487 - val_loss: 0.1839 - val_acc: 0.9279\n",
      "Epoch 12/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.1201 - acc: 0.9505 - val_loss: 0.1354 - val_acc: 0.9364\n",
      "Epoch 13/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.1026 - acc: 0.9573 - val_loss: 0.1667 - val_acc: 0.9352\n",
      "Epoch 14/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.0975 - acc: 0.9568 - val_loss: 0.1345 - val_acc: 0.9425\n",
      "Epoch 15/15\n",
      "3821/3821 [==============================] - 4s - loss: 0.0974 - acc: 0.9607 - val_loss: 0.1432 - val_acc: 0.9425\n",
      "820/820 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21692888189743204, 0.92682926800192855]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, callbacks=[earlyStopping], validation_data=(x_val, y_val), nb_epoch=15, batch_size=50)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After it, the embedding layer will be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3821 samples, validate on 818 samples\n",
      "Epoch 1/10\n",
      "3821/3821 [==============================] - 9s - loss: 0.0852 - acc: 0.9639 - val_loss: 0.1385 - val_acc: 0.9438\n",
      "Epoch 2/10\n",
      "3821/3821 [==============================] - 8s - loss: 0.0771 - acc: 0.9665 - val_loss: 0.1506 - val_acc: 0.9438\n",
      "Epoch 3/10\n",
      "3821/3821 [==============================] - 9s - loss: 0.0653 - acc: 0.9712 - val_loss: 0.1627 - val_acc: 0.9425\n",
      "Epoch 4/10\n",
      "3821/3821 [==============================] - 8s - loss: 0.0589 - acc: 0.9764 - val_loss: 0.1339 - val_acc: 0.9487\n",
      "Epoch 5/10\n",
      "3821/3821 [==============================] - 8s - loss: 0.0563 - acc: 0.9751 - val_loss: 0.1541 - val_acc: 0.9474\n",
      "Epoch 6/10\n",
      "3821/3821 [==============================] - 7s - loss: 0.0524 - acc: 0.9785 - val_loss: 0.1901 - val_acc: 0.9364\n",
      "Epoch 7/10\n",
      "3821/3821 [==============================] - 7s - loss: 0.0489 - acc: 0.9827 - val_loss: 0.1678 - val_acc: 0.9425\n",
      "Epoch 8/10\n",
      "3821/3821 [==============================] - 8s - loss: 0.0379 - acc: 0.9840 - val_loss: 0.1913 - val_acc: 0.9364\n",
      "Epoch 9/10\n",
      "3821/3821 [==============================] - 8s - loss: 0.0467 - acc: 0.9809 - val_loss: 0.1820 - val_acc: 0.9401\n",
      "Epoch 10/10\n",
      "3821/3821 [==============================] - 8s - loss: 0.0420 - acc: 0.9819 - val_loss: 0.1617 - val_acc: 0.9413\n",
      "820/820 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23516770859317082, 0.92926829268292688]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].trainable=True # This allows the embedding layer to be trained\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train, callbacks=[earlyStopping], validation_data=(x_val, y_val), nb_epoch=10, batch_size=50)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
